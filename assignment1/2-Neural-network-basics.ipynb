{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://web.stanford.edu/class/cs224n/assignment1/assignment1_soln.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is identical to logistic function. I've derived its graident multiple times. Here it is again.\n",
    "\n",
    "$$\\sigma = \\frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}$$\n",
    "\n",
    "Then,\n",
    "\\begin{align*}\n",
    "\\frac{d \\sigma}{dx} &= - (1 + e^{-x})^{-2} e^{-x} (-1) \\\\\n",
    "                                 &=\\frac{e^{-x}}{(1 + e^{-x})^2} \\\\\n",
    "                                 &=(\\frac{1}{1 + e^{-x}}) (\\frac{e^{-x}}{1 + e^{-x}}) \\\\\n",
    "                                 &=\\sigma(1 - \\sigma)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat y = \\mathrm{softmax}(\\theta) = \\begin{bmatrix}\n",
    "e^{\\theta_1} / \\sum_j e^{\\theta_j} \\\\ \n",
    "e^{\\theta_2} / \\sum_j e^{\\theta_j} \\\\ \n",
    "\\vdots \\\\ \n",
    "e^{\\theta_q} / \\sum_j e^{\\theta_j} \\\\ \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "suppose there are $q$ classes, i.e $\\hat y$ is a $q$-dimension vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "CE(y, \\hat y) = J(y, \\hat y) = 1\\{y_i = 1\\} y_i \\mathrm{log}(\\hat y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\theta_j} \n",
    "&= \\sum_{i}^{q} y_i \\frac{1}{\\hat y_i} \\frac{\\partial \\hat y_i}{\\partial \\theta_j} \\\\ \n",
    "&= 1\\{y_i = 1\\} \\frac{\\sum_k e^{-\\theta_k}}{e^{-\\theta_i}} \\frac{\\partial \\frac{e^{-\\theta_i}}{\\sum_{k} e^{-\\theta_k}}}{\\partial \\theta_j} \\\\ \n",
    "&= 1\\{y_i = 1\\} \\frac{\\sum_k e^{-\\theta_k}}{e^{-\\theta_i}} \\frac{\\frac{\\partial e^{-\\theta_i}}{\\partial \\theta_j} \\sum_k e^{-\\theta_k} - e^{-\\theta_i} \\frac{\\partial \\sum_k e^{-\\theta_k}}{\\partial \\theta_j}}{(\\sum_k e^{-\\theta_k})^2} \\\\ \n",
    "&= 1\\{y_i = 1\\} \\frac{1}{e^{-\\theta_i}} \\frac{1\\{i = j \\} (- e^{-\\theta_i}  \\sum_k e^{-\\theta_k}) - e^{-\\theta_i} (- e^{-\\theta_j})}{\\sum_k e^{-\\theta_k}} \\\\ \n",
    "&= 1\\{y_i = 1\\} -y_i \\frac{1\\{i = j \\} \\sum_k e^{-\\theta_k} - e^{-\\theta_j}}{\\sum_k e^{-\\theta_k}} \\\\ \n",
    "&= 1\\{y_i = 1\\} \\frac{e^{-\\theta_j} - 1\\{i = j \\} \\sum_k e^{-\\theta_k}}{\\sum_k e^{-\\theta_k}} \\\\ \n",
    "&= 1\\{y_i = 1\\} (\\hat y_j  - 1\\{i = j \\}) \\\\ \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\theta_1} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_i} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_q} \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\hat y_1  - 0 \\\\ \n",
    "\\vdots \\\\\n",
    "\\hat y_i - 1 \\{y_i = 1 \\} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat y_q - 0\\\\\n",
    "\\end{bmatrix} = \\hat y - y\n",
    "\\end{align*}\n",
    "\n",
    "Neat result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    "\n",
    "$$ h = sigmoid(z_1) = s(z_1) = s(x W_1 + b_1) $$\n",
    "$$ \\hat y = softmax(z_2) = f(z_2) = f(hW_2 + b_2)$$\n",
    "$$ J = CE(y, \\hat y) $$\n",
    "\n",
    "So $W_1$ is of shape $q_0 \\times q_1$, and $W_2$ is of shape $q_1 \\times q_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that using **row vector** makes the math more easy and consistent:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z_2} \\frac{\\partial z_2}{\\partial h} \\frac{\\partial h}{\\partial z_1} \\frac{\\partial z_1}{\\partial x}\n",
    "\\end{align*}\n",
    "\n",
    "The corresponding matrix shape transformation is \n",
    "\n",
    "$$\n",
    "1 \\times q_0 = (1 \\times q_2) (q_2 \\times q_2) (q_2 \\times q_1) (q_1 \\times q_1) (q_1 \\times q_0)\n",
    "$$\n",
    "\n",
    "When $q_i$ is the number of neurons in layer $i$ with $q_0$ meaning the number of neurons in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to be the case when $x$ is a **column vector**, which would be:\n",
    "\n",
    "$$q_0 \\times 1 \\ne (q_2 \\times 1) (q_2 \\times q_2) (q_1 \\times q_2) (q_1 \\times q_1) (n \\times q_1)$$\n",
    "\n",
    "or maybe my linear algebra is wrong, please let me know if you know the right way to generalize to column vectors.\n",
    "\n",
    "**Update (2017-10-24)**: see discussion on https://math.stackexchange.com/questions/2486150/jacobian-and-chain-rule. Indeed, row vector is more natural, but using colum vector is mostly about adding proper transpose operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to solve $\\frac{\\partial J}{\\partial x}$,\n",
    "\n",
    "Shape: $1 \\times q_2$,\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z_2} = \\hat y - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: $q_2 \\times q_1$,\n",
    "$$\n",
    "\\frac{\\partial z_2}{\\partial h} = W_2^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: $q_1 \\times q_1$ (the second equality HASN'T been proved yet),\n",
    "\n",
    "$$\\frac{\\partial h}{\\partial z_1} = \\sigma '(z_1) = h(z_1)(1 - h(z_1))^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: $q_1 \\times q_0$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial z_1}{\\partial x} = W_1^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_1 &= \\frac{\\partial J}{\\partial z_2} = \\hat y - y \\\\\n",
    "\\delta_2 &= \\frac{\\partial J}{\\partial h}  = (\\hat y - y)W_2^T \\\\\n",
    "\\delta_3 &= \\frac{\\partial J}{\\partial z_1} = (\\hat y - y) W_2^T \\sigma'(z_1)\\\\\n",
    "\\frac{\\partial J}{\\partial x} &= (\\hat y - y) W_2^T \\sigma'(z_1) W_1^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** what does the empty circle ($\\circ$) mean in $\\delta_3 = \\delta_2 \\circ \\sigma'(z_1)$ mean? In the solution file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $(D_x H + H) + (H D_y + D_y)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e), (f), (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are programming exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
